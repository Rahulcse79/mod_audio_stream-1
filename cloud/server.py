%%writefile server.py
import numpy as np
import whisper
import torch
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from transformers import AutoTokenizer, AutoModelForCausalLM

app = FastAPI()

print("üîÅ Loading STT model...")
stt = whisper.load_model("base")

print("üîÅ Loading LLM model...")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
tokenizer.pad_token = tokenizer.eos_token

llm = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    device_map="auto",
    torch_dtype=torch.float16
)
llm.config.pad_token_id = tokenizer.pad_token_id

print("‚úÖ Models loaded successfully")

@app.websocket("/ws")
async def ws_endpoint(ws: WebSocket):
    await ws.accept()
    print("üîå WebSocket connected")
    audio_chunks = []

    stop_received = False
    try:
        while True:
            msg = await ws.receive()
            msg_type = msg.get("type", "")

            # Client disconnected before sending STOP
            if msg_type == "websocket.disconnect":
                print("üîå Client disconnected before sending STOP")
                return

            # Client sent STOP text signal ‚Äî done recording
            if msg_type == "websocket.receive" and msg.get("text") == "STOP":
                print("üì© Received STOP signal from client")
                stop_received = True
                break

            # Binary audio data
            if msg_type == "websocket.receive" and msg.get("bytes"):
                data = msg["bytes"]
                audio = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                audio_chunks.append(audio)

    except WebSocketDisconnect:
        print("üîå Client disconnected unexpectedly")
        return
    except Exception as e:
        print(f"‚ùå Error receiving data: {e}")
        return

    if not stop_received:
        return

    # --- Process audio after STOP signal (connection is still open) ---

    if not audio_chunks:
        print("‚ö†Ô∏è No audio received")
        await ws.send_text("ERROR: No audio received")
        await ws.close()
        return

    audio = np.concatenate(audio_chunks)
    print(f"üéß Received audio samples: {len(audio)}")

    # ---------- STT ----------
    result = stt.transcribe(
        audio,
        fp16=torch.cuda.is_available()
    )
    user_text = result.get("text", "").strip()

    if not user_text:
        print("‚ö†Ô∏è Empty transcription")
        await ws.send_text("ERROR: Could not transcribe audio")
        await ws.close()
        return

    print("üó£ User:", user_text)

    # ---------- LLM ----------
    prompt = f"User: {user_text}\nAI:"
    inputs = tokenizer(prompt, return_tensors="pt").to(llm.device)

    output = llm.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )

    ai_text = tokenizer.decode(output[0], skip_special_tokens=True)
    ai_text = ai_text.split("AI:")[-1].strip()

    print("ü§ñ AI:", ai_text)

    # Send response back (connection is still open!)
    try:
        await ws.send_text(ai_text)
        await ws.close()
        print("‚úÖ Response sent, connection closed")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not send response (client may have disconnected): {e}")
